{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    \n",
    "    def pre_processing(self,dataset):\n",
    "        #print(dataset.isnull().sum())\n",
    "        dataset['Gender'] = dataset['Gender'].map({'Male':1,'Female':0})\n",
    "        dataset['Married'] = dataset['Married'].map({'Yes':1,'No':0})\n",
    "        dataset['Education'] = dataset['Education'].map({'Graduate':1,'Not Graduate':0})\n",
    "        dataset['Self_Employed'] = dataset['Self_Employed'].map({'Yes':1,'No':0})\n",
    "        dataset['Property_Area'] = dataset['Property_Area'].map({'Urban':1,'Rural':0,'Semiurban':2})\n",
    "        dataset['Dependents'] = dataset['Dependents'].map({'0':1,'1':0,'2':2,'3+':3})\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def adjust_null(self,dataset):\n",
    "        '''\n",
    "        dataset['Gender'].fillna('Male', inplace=True)\n",
    "        dataset['Married'].fillna('Yes', inplace=True)\n",
    "        dataset['Dependents'].fillna(0, inplace=True)\n",
    "        dataset['Self_Employed'].fillna('No', inplace=True)\n",
    "        dataset['Credit_History'].fillna(2, inplace=True)\n",
    "        '''\n",
    "        dataset['LoanAmount'].fillna(dataset['LoanAmount'].median(), inplace = True)\n",
    "        dataset['Loan_Amount_Term'].fillna(dataset['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "        \n",
    "        dataset['Gender'].fillna(dataset['Gender'].mode()[0], inplace=True)\n",
    "        dataset['Married'].fillna(dataset['Married'].mode()[0], inplace=True)\n",
    "        dataset['Dependents'].fillna(dataset['Dependents'].mode()[0], inplace=True)\n",
    "        dataset['Self_Employed'].fillna(dataset['Self_Employed'].mode()[0], inplace=True)\n",
    "        dataset['Credit_History'].fillna(dataset['Credit_History'].mode()[0], inplace=True)\n",
    "        \n",
    "        #print(dataset.isnull().sum())\n",
    "        return dataset\n",
    "        \n",
    "        \n",
    "        \n",
    "    def data_analysis(self):\n",
    "        sns.barplot(x = 'Gender', y = 'Loan_Status', data=self.train)\n",
    "        sns.barplot(x = 'ApplicantIncome', y = 'Loan_Status',data=self.train)\n",
    "        sns.barplot(x = 'Self_Employed', y = 'Loan_Status', data=self.train)\n",
    "        #plt.scatter(self.x_train, self.y_train)\n",
    "        #plt.show()\n",
    "        \n",
    "    def get_preprocess_data(self,data):\n",
    "        dataset = self.pre_processing(data)\n",
    "        dataset = self.adjust_null(dataset)\n",
    "        #dataset['Loan_Status'] = self.label_encoder.fit_transform(dataset['Loan_Status'])\n",
    "        dataset['Loan_Status'] = dataset['Loan_Status'].map({'Y':1,'N':0})\n",
    "        #print(dataset.head())\n",
    "        #print(len(dataset))\n",
    "        dataset = dataset.dropna(axis=0)\n",
    "        dataset = dataset.drop('Loan_ID',axis=1)\n",
    "        #print(dataset.isnull().sum())\n",
    "        #print(len(dataset))\n",
    "        self.train,self.test = train_test_split(dataset,test_size = 0.1,random_state = 0,shuffle=False)\n",
    "        \n",
    "        self.y_train = self.train['Loan_Status']\n",
    "        self.x_train = self.train.drop('Loan_Status',axis=1)\n",
    "        \n",
    "        \n",
    "        self.y_test = self.test['Loan_Status']\n",
    "        self.x_test = self.test.drop('Loan_Status',axis=1)\n",
    "        return dataset\n",
    "        #print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.x_train = preprocess_obj.x_train\n",
    "        self.y_train = preprocess_obj.y_train\n",
    "        self.x_test = preprocess_obj.x_test\n",
    "        self.y_test = preprocess_obj.y_test\n",
    "        \n",
    "    def train(self):\n",
    "        self.model1 = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "        \n",
    "        self.model2 = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "\n",
    "        self.model3 = LogisticRegression(class_weight='balanced')\n",
    "        \n",
    "        #self.model4 = xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "        self.model4 = xgb.XGBClassifier(random_state=1, n_estimators=81, max_depth=1)\n",
    "\n",
    "        self.model5 = AdaBoostClassifier(random_state=1,n_estimators=5)\n",
    "\n",
    "        self.model6 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "        \n",
    "        self.model7 = MLPClassifier(hidden_layer_sizes=(3), max_iter=100)\n",
    "        \n",
    "        self.model8 = RandomForestClassifier(random_state=1, max_depth=10, n_estimators=10)\n",
    "\n",
    "        self.model1.fit(self.x_train,self.y_train)\n",
    "        print(\"Bagging: \")\n",
    "        print(self.model1.score(self.x_test,self.y_test))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.model2.fit(self.x_train,self.y_train)\n",
    "        print(\"Gradient Boosting: \")\n",
    "        print(self.model2.score(self.x_test,self.y_test))\n",
    "        self.model2.fit(self.x_test,self.y_test)\n",
    "        print(self.model2.score(self.x_test,self.y_test))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.model3.fit(self.x_train,self.y_train)\n",
    "        print(\"Logistic Regression: \")\n",
    "        print(self.model3.score(self.x_test,self.y_test))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"XGB: \")\n",
    "        self.model4.fit(self.x_train,self.y_train)\n",
    "        print(self.model4.score(self.x_test,self.y_test))\n",
    "        self.model4.fit(self.x_test,self.y_test)\n",
    "        print(self.model4.score(self.x_test,self.y_test))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"AdaBoost: \")\n",
    "        self.model5.fit(self.x_train,self.y_train)\n",
    "        print(self.model5.score(self.x_test,self.y_test))\n",
    "        self.model5.fit(self.x_test,self.y_test)\n",
    "        print(self.model5.score(self.x_test,self.y_test))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"KNN: \")\n",
    "        self.model6.fit(self.x_train,self.y_train)\n",
    "        print(self.model6.score(self.x_test,self.y_test))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"MLP: \")\n",
    "        self.model7.fit(self.x_train,self.y_train)\n",
    "        print(self.model7.score(self.x_test,self.y_test))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"Random Forest: \")\n",
    "        self.model8.fit(self.x_train,self.y_train)\n",
    "        print(self.model8.score(self.x_test,self.y_test))\n",
    "        \n",
    "    def nn_model1(self):\n",
    "        n_cols = self.x_train.shape[1]\n",
    "        self.model = keras.models.Sequential()\n",
    "    \n",
    "        self.model.add(keras.layers.Dense(256, kernel_initializer=\"truncated_normal\", activation = \"relu\", input_shape = (n_cols,)))\n",
    "        self.model.add(keras.layers.Dropout(0.1))\n",
    "    \n",
    "        self.model.add(keras.layers.Dense(128, kernel_initializer=\"truncated_normal\", activation = \"relu\", input_shape = (n_cols,)))\n",
    "        self.model.add(keras.layers.Dropout(0.1))\n",
    "    \n",
    "        self.model.add(keras.layers.Dense(64, kernel_initializer=\"truncated_normal\", activation = \"relu\", input_shape = (n_cols,)))\n",
    "        self.model.add(keras.layers.Dropout(0.1))\n",
    "    \n",
    "        self.model.add(keras.layers.Dense(1, kernel_initializer=\"truncated_normal\", activation = \"sigmoid\" ))\n",
    "    \n",
    "        self.model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [\"accuracy\"])\n",
    "        self.model.fit(self.x_train,self.y_train, batch_size = 10, epochs=50, validation_data = (self.x_test,self.y_test))\n",
    "    \n",
    "    def nn_model2(self):\n",
    "        n_cols = self.x_train.shape[1]\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.Dense(128, activation='relu', input_shape = (n_cols,)))\n",
    "        self.model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        self.model.add(keras.layers.Dense(32, activation='relu'))\n",
    "        self.model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        self.model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        self.model.fit(self.x_train,self.y_train, batch_size = 5,epochs=50, validation_data = (self.x_test,self.y_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join('Data','train.csv'))\n",
    "preprocess_obj = Preprocessing()\n",
    "train_data = preprocess_obj.get_preprocess_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging: \n",
      "0.8225806451612904\n",
      "\n",
      "\n",
      "Gradient Boosting: \n",
      "0.8387096774193549\n",
      "0.9193548387096774\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n",
      "0.7741935483870968\n",
      "\n",
      "\n",
      "XGB: \n",
      "0.8387096774193549\n",
      "0.9516129032258065\n",
      "\n",
      "\n",
      "AdaBoost: \n",
      "0.8548387096774194\n",
      "0.8870967741935484\n",
      "\n",
      "\n",
      "KNN: \n",
      "0.5806451612903226\n",
      "\n",
      "\n",
      "MLP: \n",
      "0.7096774193548387\n",
      "\n",
      "\n",
      "Random Forest: \n",
      "0.7741935483870968\n"
     ]
    }
   ],
   "source": [
    "model_obj = GenModel()\n",
    "model_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 552 samples, validate on 62 samples\n",
      "Epoch 1/50\n",
      "552/552 [==============================] - 0s 828us/sample - loss: 39.9995 - accuracy: 0.5688 - val_loss: 9.8522 - val_accuracy: 0.5323\n",
      "Epoch 2/50\n",
      "552/552 [==============================] - 0s 260us/sample - loss: 19.3756 - accuracy: 0.5471 - val_loss: 19.0642 - val_accuracy: 0.4355\n",
      "Epoch 3/50\n",
      "552/552 [==============================] - 0s 265us/sample - loss: 16.6818 - accuracy: 0.5598 - val_loss: 35.9035 - val_accuracy: 0.3710\n",
      "Epoch 4/50\n",
      "552/552 [==============================] - 0s 260us/sample - loss: 14.4795 - accuracy: 0.5815 - val_loss: 11.2875 - val_accuracy: 0.5645\n",
      "Epoch 5/50\n",
      "552/552 [==============================] - 0s 254us/sample - loss: 7.5805 - accuracy: 0.5942 - val_loss: 6.0655 - val_accuracy: 0.7097\n",
      "Epoch 6/50\n",
      "552/552 [==============================] - 0s 336us/sample - loss: 6.2644 - accuracy: 0.6214 - val_loss: 12.3674 - val_accuracy: 0.4355\n",
      "Epoch 7/50\n",
      "552/552 [==============================] - 0s 273us/sample - loss: 13.2077 - accuracy: 0.5543 - val_loss: 25.1432 - val_accuracy: 0.6613\n",
      "Epoch 8/50\n",
      "552/552 [==============================] - 0s 261us/sample - loss: 5.3276 - accuracy: 0.6087 - val_loss: 2.3391 - val_accuracy: 0.6935\n",
      "Epoch 9/50\n",
      "552/552 [==============================] - 0s 241us/sample - loss: 3.0950 - accuracy: 0.6123 - val_loss: 7.4471 - val_accuracy: 0.6613\n",
      "Epoch 10/50\n",
      "552/552 [==============================] - 0s 268us/sample - loss: 4.8832 - accuracy: 0.5960 - val_loss: 35.5392 - val_accuracy: 0.3387\n",
      "Epoch 11/50\n",
      "552/552 [==============================] - 0s 245us/sample - loss: 27.8747 - accuracy: 0.5580 - val_loss: 6.5229 - val_accuracy: 0.3387\n",
      "Epoch 12/50\n",
      "552/552 [==============================] - 0s 260us/sample - loss: 7.5893 - accuracy: 0.5580 - val_loss: 10.9958 - val_accuracy: 0.6613\n",
      "Epoch 13/50\n",
      "552/552 [==============================] - 0s 331us/sample - loss: 3.9352 - accuracy: 0.5652 - val_loss: 3.1756 - val_accuracy: 0.6613\n",
      "Epoch 14/50\n",
      "552/552 [==============================] - 0s 242us/sample - loss: 1.9188 - accuracy: 0.5761 - val_loss: 2.6319 - val_accuracy: 0.4032\n",
      "Epoch 15/50\n",
      "552/552 [==============================] - 0s 251us/sample - loss: 1.3109 - accuracy: 0.5906 - val_loss: 1.5290 - val_accuracy: 0.6613\n",
      "Epoch 16/50\n",
      "552/552 [==============================] - 0s 268us/sample - loss: 0.8713 - accuracy: 0.5978 - val_loss: 1.1180 - val_accuracy: 0.6774\n",
      "Epoch 17/50\n",
      "552/552 [==============================] - 0s 261us/sample - loss: 0.9457 - accuracy: 0.5978 - val_loss: 0.7818 - val_accuracy: 0.6290\n",
      "Epoch 18/50\n",
      "552/552 [==============================] - 0s 251us/sample - loss: 1.4158 - accuracy: 0.6178 - val_loss: 1.5847 - val_accuracy: 0.6613\n",
      "Epoch 19/50\n",
      "552/552 [==============================] - 0s 301us/sample - loss: 0.9572 - accuracy: 0.6286 - val_loss: 0.8551 - val_accuracy: 0.6774\n",
      "Epoch 20/50\n",
      "552/552 [==============================] - 0s 272us/sample - loss: 1.6398 - accuracy: 0.5797 - val_loss: 2.1041 - val_accuracy: 0.6613\n",
      "Epoch 21/50\n",
      "552/552 [==============================] - 0s 402us/sample - loss: 1.1494 - accuracy: 0.6214 - val_loss: 2.0992 - val_accuracy: 0.4516\n",
      "Epoch 22/50\n",
      "552/552 [==============================] - 0s 344us/sample - loss: 1.1073 - accuracy: 0.6322 - val_loss: 1.4449 - val_accuracy: 0.6613\n",
      "Epoch 23/50\n",
      "552/552 [==============================] - 0s 393us/sample - loss: 1.0210 - accuracy: 0.6087 - val_loss: 1.5785 - val_accuracy: 0.6613\n",
      "Epoch 24/50\n",
      "552/552 [==============================] - 0s 481us/sample - loss: 1.0550 - accuracy: 0.6341 - val_loss: 0.9194 - val_accuracy: 0.4355\n",
      "Epoch 25/50\n",
      "552/552 [==============================] - 0s 270us/sample - loss: 0.8855 - accuracy: 0.6286 - val_loss: 0.5832 - val_accuracy: 0.5968\n",
      "Epoch 26/50\n",
      "552/552 [==============================] - 0s 264us/sample - loss: 0.7918 - accuracy: 0.6196 - val_loss: 0.6992 - val_accuracy: 0.6613\n",
      "Epoch 27/50\n",
      "552/552 [==============================] - 0s 273us/sample - loss: 0.7406 - accuracy: 0.6395 - val_loss: 0.7043 - val_accuracy: 0.6935\n",
      "Epoch 28/50\n",
      "552/552 [==============================] - 0s 269us/sample - loss: 0.7760 - accuracy: 0.6322 - val_loss: 0.7732 - val_accuracy: 0.6774\n",
      "Epoch 29/50\n",
      "552/552 [==============================] - 0s 264us/sample - loss: 0.7524 - accuracy: 0.6449 - val_loss: 1.6636 - val_accuracy: 0.6613\n",
      "Epoch 30/50\n",
      "552/552 [==============================] - 0s 258us/sample - loss: 0.7773 - accuracy: 0.6395 - val_loss: 0.5952 - val_accuracy: 0.6774\n",
      "Epoch 31/50\n",
      "552/552 [==============================] - 0s 339us/sample - loss: 0.7464 - accuracy: 0.6268 - val_loss: 1.0962 - val_accuracy: 0.6774\n",
      "Epoch 32/50\n",
      "552/552 [==============================] - 0s 247us/sample - loss: 0.8253 - accuracy: 0.6449 - val_loss: 0.8087 - val_accuracy: 0.5806\n",
      "Epoch 33/50\n",
      "552/552 [==============================] - 0s 241us/sample - loss: 0.7500 - accuracy: 0.6486 - val_loss: 1.0225 - val_accuracy: 0.6613\n",
      "Epoch 34/50\n",
      "552/552 [==============================] - 0s 252us/sample - loss: 0.7676 - accuracy: 0.6467 - val_loss: 0.6647 - val_accuracy: 0.6774\n",
      "Epoch 35/50\n",
      "552/552 [==============================] - 0s 411us/sample - loss: 0.6897 - accuracy: 0.6667 - val_loss: 0.6422 - val_accuracy: 0.6935\n",
      "Epoch 36/50\n",
      "552/552 [==============================] - 0s 307us/sample - loss: 0.6339 - accuracy: 0.6703 - val_loss: 0.6161 - val_accuracy: 0.7097\n",
      "Epoch 37/50\n",
      "552/552 [==============================] - 0s 355us/sample - loss: 0.6324 - accuracy: 0.6812 - val_loss: 0.6084 - val_accuracy: 0.6935\n",
      "Epoch 38/50\n",
      "552/552 [==============================] - 0s 237us/sample - loss: 0.6480 - accuracy: 0.6830 - val_loss: 0.6078 - val_accuracy: 0.7419\n",
      "Epoch 39/50\n",
      "552/552 [==============================] - 0s 255us/sample - loss: 0.6449 - accuracy: 0.6721 - val_loss: 0.6789 - val_accuracy: 0.6774\n",
      "Epoch 40/50\n",
      "552/552 [==============================] - 0s 260us/sample - loss: 0.6456 - accuracy: 0.6739 - val_loss: 0.6347 - val_accuracy: 0.6935\n",
      "Epoch 41/50\n",
      "552/552 [==============================] - 0s 321us/sample - loss: 0.7093 - accuracy: 0.6522 - val_loss: 0.8208 - val_accuracy: 0.6774\n",
      "Epoch 42/50\n",
      "552/552 [==============================] - 0s 452us/sample - loss: 1.0147 - accuracy: 0.6431 - val_loss: 1.5142 - val_accuracy: 0.6774\n",
      "Epoch 43/50\n",
      "552/552 [==============================] - 0s 324us/sample - loss: 0.7907 - accuracy: 0.6087 - val_loss: 1.0059 - val_accuracy: 0.6613\n",
      "Epoch 44/50\n",
      "552/552 [==============================] - 0s 280us/sample - loss: 0.6425 - accuracy: 0.6848 - val_loss: 0.6367 - val_accuracy: 0.6935\n",
      "Epoch 45/50\n",
      "552/552 [==============================] - 0s 261us/sample - loss: 0.6413 - accuracy: 0.6866 - val_loss: 1.0215 - val_accuracy: 0.6613\n",
      "Epoch 46/50\n",
      "552/552 [==============================] - 0s 243us/sample - loss: 0.6545 - accuracy: 0.6649 - val_loss: 0.6198 - val_accuracy: 0.7258\n",
      "Epoch 47/50\n",
      "552/552 [==============================] - 0s 337us/sample - loss: 0.6387 - accuracy: 0.6757 - val_loss: 0.8701 - val_accuracy: 0.6613\n",
      "Epoch 48/50\n",
      "552/552 [==============================] - 0s 321us/sample - loss: 0.6448 - accuracy: 0.6848 - val_loss: 0.6149 - val_accuracy: 0.6935\n",
      "Epoch 49/50\n",
      "552/552 [==============================] - 0s 356us/sample - loss: 0.6365 - accuracy: 0.6793 - val_loss: 0.7615 - val_accuracy: 0.6613\n",
      "Epoch 50/50\n",
      "552/552 [==============================] - 0s 254us/sample - loss: 0.6291 - accuracy: 0.6848 - val_loss: 0.6223 - val_accuracy: 0.6935\n"
     ]
    }
   ],
   "source": [
    "model_obj.nn_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Gender Married Dependents     Education Self_Employed  \\\n",
      "Loan_ID                                                          \n",
      "LP001015   Male     Yes          0      Graduate            No   \n",
      "LP001022   Male     Yes          1      Graduate            No   \n",
      "LP001031   Male     Yes          2      Graduate            No   \n",
      "LP001035   Male     Yes          2      Graduate            No   \n",
      "LP001051   Male      No          0  Not Graduate            No   \n",
      "\n",
      "          ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "Loan_ID                                                                      \n",
      "LP001015             5720                  0       110.0             360.0   \n",
      "LP001022             3076               1500       126.0             360.0   \n",
      "LP001031             5000               1800       208.0             360.0   \n",
      "LP001035             2340               2546       100.0             360.0   \n",
      "LP001051             3276                  0        78.0             360.0   \n",
      "\n",
      "          Credit_History Property_Area  \n",
      "Loan_ID                                 \n",
      "LP001015             1.0         Urban  \n",
      "LP001022             1.0         Urban  \n",
      "LP001031             1.0         Urban  \n",
      "LP001035             NaN         Urban  \n",
      "LP001051             1.0         Urban  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gender               11\n",
       "Married               0\n",
       "Dependents           10\n",
       "Education             0\n",
       "Self_Employed        23\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount            5\n",
       "Loan_Amount_Term      6\n",
       "Credit_History       29\n",
       "Property_Area         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(os.path.join('Data','test.csv'),index_col='Loan_ID') \n",
    "print(test_data.head())\n",
    "test_data.isnull().sum()\n",
    "\n",
    "#test_data['Dependents'].unique() \n",
    "#pd.unique(test_data[['Dependents', 'Loan_Amount_Term']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction():\n",
    "    def __init__(self):\n",
    "        self.model = model_obj.model4\n",
    "        \n",
    "    def adjust_null(self,train,test):\n",
    "        test['Gender'].fillna(train['Gender'].mode()[0], inplace=True)\n",
    "        test['Dependents'].fillna(train['Dependents'].mode()[0], inplace=True)\n",
    "        test['Self_Employed'].fillna(train['Self_Employed'].mode()[0], inplace=True)\n",
    "        test['Credit_History'].fillna(train['Credit_History'].mode()[0], inplace=True)\n",
    "        test['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "        test['LoanAmount'].fillna(train['LoanAmount'].median(), inplace=True)\n",
    "        return test\n",
    "    def prediction(self,test_data):\n",
    "        test_data = preprocess_obj.pre_processing(test_data)\n",
    "        test_data = self.adjust_null(train_data,test_data)\n",
    "        print(test_data.isnull().sum())\n",
    "\n",
    "        \n",
    "        y_predict=self.model.predict(test_data)\n",
    "        y_predict=pd.DataFrame(y_predict,columns =['Loan_Status'],index=test_data.index)\n",
    "        y_predict['Loan_Status']= y_predict['Loan_Status'].map({1: 'Y', 0: 'N'})\n",
    "        y_predict.to_csv(\"Submission.csv\")\n",
    "        print(y_predict.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender               0\n",
      "Married              0\n",
      "Dependents           0\n",
      "Education            0\n",
      "Self_Employed        0\n",
      "ApplicantIncome      0\n",
      "CoapplicantIncome    0\n",
      "LoanAmount           0\n",
      "Loan_Amount_Term     0\n",
      "Credit_History       0\n",
      "Property_Area        0\n",
      "dtype: int64\n",
      "         Loan_Status\n",
      "Loan_ID             \n",
      "LP001015           Y\n",
      "LP001022           Y\n",
      "LP001031           Y\n",
      "LP001035           N\n",
      "LP001051           Y\n"
     ]
    }
   ],
   "source": [
    "pred_obj = Prediction()\n",
    "pred_obj.prediction(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
